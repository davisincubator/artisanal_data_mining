{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "###### models ######\n",
    "class CNN: #going to define a  class for our model\n",
    "    #and this whole thing, we\n",
    "  ## todo - implement predict function\n",
    "  def __init__(self, params): #and we're going to have parameters in the other file.\n",
    "    \"\"\"params has width, depth, numParam\"\"\"\n",
    "    self.params = params #the parameters we input are going to be the parameters of the netwrok\n",
    "    self.network_name = 'CNN' #just a name\n",
    "    self.sess = tf.Session() #no idea\n",
    "\n",
    "    self.In = tf.placeholder('float', [None, params['width'], params['height'], 4],\n",
    "                            name='In') #initializing In?\n",
    "    self.Labels = tf.placeholder('float', [None, params['numParam']],name='Labels') #initializing Labels??\n",
    "\n",
    "    # Layer 1 (Convolutional)\n",
    "    layer_name = 'conv1' #so this is a layer. And we have a conv layer to start\n",
    "    size = 10 #this is the size of the filter, it's a 10x10 pixel box\n",
    "    channels = 4 #4 channels for RBG and infra\n",
    "    filters = 32 #these are UNINITIALIZED, RANDOM hidden nodes in our convolutional layer. They will be modified by backpropogation. They may not be human readable.\n",
    "    stride = 5 #this is by how much our filter steps as it convolves the image\n",
    "    self.w1 = tf.Variable(tf.random_normal([size, size, channels, filters], stddev=0.01),\n",
    "                          name=self.network_name + '_' + layer_name + '_weights') #these are the weights, I assume, that we get out of our layers\n",
    "    self.b1 = tf.Variable(tf.constant(0.1, shape=[filters]), name=self.network_name + '_' + layer_name + '_biases') #what's a bias versus a weight? as in B from mX + B\n",
    "    self.c1 = tf.nn.conv2d(self.In, self.w1, strides=[1, stride, stride, 1], padding='SAME',\n",
    "                           name=self.network_name + '_' + layer_name + '_convs') #????\n",
    "    self.o1 = tf.nn.relu(tf.add(self.c1, self.b1), name=self.network_name + '_' + layer_name + '_activations') #???\n",
    " \n",
    "    # Layer 2 (Convolutional)\n",
    "    layer_name = 'conv2' # a second convolutional layer\n",
    "    size = 10\n",
    "    channels = 32 #32 channels cuz 32 filters in previous layer\n",
    "    filters = 64\n",
    "    stride = 5\n",
    "    self.w2 = tf.Variable(tf.random_normal([size, size, channels, filters], stddev=0.01),\n",
    "                          name=self.network_name + '_' + layer_name + '_weights')\n",
    "    self.b2 = tf.Variable(tf.constant(0.1, shape=[filters]), name=self.network_name + '_' + layer_name + '_biases')\n",
    "    self.c2 = tf.nn.conv2d(self.o1, self.w2, strides=[1, stride, stride, 1], padding='SAME',\n",
    "                           name=self.network_name + '_' + layer_name + '_convs')\n",
    "    self.o2 = tf.nn.relu(tf.add(self.c2, self.b2), name=self.network_name + '_' + layer_name + '_activations')\n",
    "\n",
    "    o2_shape = self.o2.get_shape().as_list()\n",
    "\n",
    "    # Layer 3 (Fully connected) #so now we have a fully connected layer, which is just a straight up neural network layer... I think it is\n",
    "    #that these filters, these twice-convolved feature\n",
    "    layer_name = 'fc3'\n",
    "    hiddens = 256\n",
    "    dim = o2_shape[1] * o2_shape[2] * o2_shape[3]\n",
    "    self.o2_flat = tf.reshape(self.o2, [-1, dim], name=self.network_name + '_' + layer_name + '_input_flat')\n",
    "    self.w3 = tf.Variable(tf.random_normal([dim, hiddens], stddev=0.01),\n",
    "                          name=self.network_name + '_' + layer_name + '_weights')\n",
    "    self.b3 = tf.Variable(tf.constant(0.1, shape=[hiddens]), name=self.network_name + '_' + layer_name + '_biases')\n",
    "    self.ip3 = tf.add(tf.matmul(self.o2_flat, self.w3), self.b3, name=self.network_name + '_' + layer_name + '_ips')\n",
    "    self.o3 = tf.nn.relu(self.ip3, name=self.network_name + '_' + layer_name + '_activations')\n",
    "\n",
    "    # Layer 4 (Fully connected output)\n",
    "    layer_name = 'fc4'\n",
    "    hiddens = params['numParam']\n",
    "    dim = 256\n",
    "    self.w4 = tf.Variable(tf.random_normal([dim, hiddens], stddev=0.01),\n",
    "                          name=self.network_name + '_' + layer_name + '_weights')\n",
    "    self.b4 = tf.Variable(tf.constant(0.1, shape=[hiddens]), name=self.network_name + '_' + layer_name + '_biases')\n",
    "    self.logits = tf.add(tf.matmul(self.o3, self.w4), self.b4, name='logits')\n",
    "    self.Out = tf.nn.sigmoid(self.logits, name='Out')\n",
    "\n",
    "    # Cost,Optimizer\n",
    "    #self.cost = tf.reduce_sum(tf.pow(tf.subtract(self.Out, self.Labels), 2))\n",
    "    self.cost = tf.nn.sigmoid_cross_entropy_with_logits(logits=self.logits, labels=self.Labels)\n",
    "\n",
    "    if self.params['load_file'] is not None:\n",
    "      self.global_step = tf.Variable(int(self.params['load_file'].split('_')[-1]), name='global_step', trainable=False)\n",
    "    else:\n",
    "      self.global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "\n",
    "    # Gradient descent on loss function\n",
    "    self.rmsprop = tf.train.RMSPropOptimizer(self.params['lr'], epsilon=self.params['rms_eps']).minimize(self.cost,\n",
    "                                                                                                         global_step=self.global_step)\n",
    "\n",
    "    self.saver = tf.train.Saver(max_to_keep=0)\n",
    "\n",
    "    self.sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    if self.params['load_file'] is not None:\n",
    "      print('Loading checkpoint...')\n",
    "      self.saver.restore(self.sess, self.params['load_file'])\n",
    "\n",
    "  def train(self, bat_In, labels):\n",
    "    feed_dict = {self.In: bat_In, self.Labels: labels}\n",
    "    _, cnt, cost = self.sess.run([self.rmsprop, self.global_step, self.cost], feed_dict=feed_dict)\n",
    "    return cnt, cost/bat_In.shape[0]\n",
    "\n",
    "  def predict(self,bat_In):\n",
    "    feed_dict = {self.In: bat_In}\n",
    "    out = self.sess.run([self.Out], feed_dict=feed_dict)\n",
    "    prediction = np.round(out)\n",
    "    return prediction\n",
    "\n",
    "  def save_ckpt(self, filename):\n",
    "    self.saver.save(self.sess, filename)\n",
    "\n",
    "###### helper functions ######\n",
    "def upsample_layer(bottom,\n",
    "               n_channels, name, upscale_factor):\n",
    "\n",
    "  kernel_size = 2 * upscale_factor - upscale_factor % 2\n",
    "  stride = upscale_factor\n",
    "  strides = [1, stride, stride, 1]\n",
    "  with tf.variable_scope(name):\n",
    "    # Shape of the bottom tensor\n",
    "    in_shape = tf.shape(bottom)\n",
    "\n",
    "    h = ((in_shape[1] - 1) * stride) + 1\n",
    "    w = ((in_shape[2] - 1) * stride) + 1\n",
    "    new_shape = [in_shape[0], h, w, n_channels]\n",
    "    output_shape = tf.stack(new_shape)\n",
    "\n",
    "    filter_shape = [kernel_size, kernel_size, n_channels, n_channels]\n",
    "\n",
    "    weights = get_bilinear_filter(filter_shape, upscale_factor)\n",
    "    deconv = tf.nn.conv2d_transpose(bottom, weights, output_shape,\n",
    "                                    strides=strides, padding='SAME')\n",
    "\n",
    "  return deconv"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
